{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58afdc07-db6b-42b7-8296-40c28d6c1e62",
   "metadata": {},
   "source": [
    "## The DeepONet method\n",
    "\n",
    "- [DeepONet](https://odi.inf.ethz.ch/teaching/AI4Science/Group11.pdf)\n",
    "- [DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators](https://arxiv.org/pdf/1910.03193)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ba55d-5e34-4cf4-a6bd-f5f0d3662300",
   "metadata": {},
   "source": [
    "## (1) What is DeepONet? — The DeepONet Architecture\n",
    "\n",
    "DeepONet is one of the foundational architectures in the family of **deep neural operators (DNOs)**, designed specifically to learn operators that map input functions to output functions. Unlike standard neural networks that map vectors to vectors, DeepONet learns a **function-to-function mapping** by cleverly separating the handling of input functions and spatial coordinates.\n",
    "\n",
    "### (1.1) The Core Idea\n",
    "\n",
    "The DeepONet architecture consists of two neural networks working in parallel:\n",
    "- **Branch network**: Encodes the input function (e.g., a coefficient field like permeability or initial condition).\n",
    "- **Trunk network**: Encodes the evaluation location (e.g., a spatial point $x$, or spatial-temporal point $(x,t)$) at which we want to evaluate the output solution.\n",
    "\n",
    "Together, these two components produce an approximation of the solution operator:\n",
    "$\\mathcal{G}_\\theta(a)(x) \\approx u(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fad29-f12b-4c51-823c-7b18c7b80b97",
   "metadata": {},
   "source": [
    "### (1.2) Structure of DeepONet\n",
    "\n",
    "The structure of the DeepONet architecture is illustrated in Figure 1. Let’s break down each component:\n",
    "<center>\n",
    "    <img src=\"./Figures/DeepONet.png\" width = \"800\" height='600' alt=\"\"/>\n",
    "    <br>\n",
    "    <div style=\"color:gray\">\n",
    "      Fig 1: The architecture of DeepONet\n",
    "  \t</div>\n",
    "</center>\n",
    "\n",
    "#### Branch Network — Representing the Input Function\n",
    "\n",
    "The branch network takes a finite representation of the input function $a(x)$, which cannot be directly input into a neural network. There are two common approaches to obtain this representation:\n",
    "- **Basis Expansion Representation**:\n",
    "  - Represent the function as a sum of basis functions:\n",
    "   $$\n",
    "   a(x) = \\sum_i a_i \\phi_i(x)\n",
    "   $$\n",
    "  - Use the first $m$ coefficients ($a_1, a_2, \\ldots, a_m$) as input to the branch network.\n",
    "-  **Sensor-based Discretization**:\n",
    "\t- Sample the function values at a set of predefined sensor points $\\Xi = \\{\\xi_1, \\xi_2, \\ldots, \\xi_m\\}$:\n",
    "   $$\n",
    "    a(\\Xi) = (a(\\xi_1), a(\\xi_2), \\ldots, a(\\xi_m))\n",
    "   $$\n",
    "\t- This vector of sampled values serves as the input to the branch network.\n",
    "\n",
    "The branch network processes this input and outputs a set of p intermediate terms:\n",
    "$$b = (b_1(a), b_2(a), \\ldots, b_p(a)).$$\n",
    "\n",
    "#### Trunk Network — Representing the Spatial Coordinates\n",
    "\n",
    "The trunk network takes the evaluation point $x \\in \\Omega$ as input and also outputs $p$ intermediate terms:\n",
    "$$t = (t_1(x), t_2(x), \\ldots, t_p(x)).$$\n",
    "\n",
    "These terms encode information about where in the domain the solution is being evaluated.\n",
    "\n",
    "#### Combining the Outputs\n",
    "\n",
    "Finally, the outputs of the branch and trunk networks are combined via an inner product, followed by the addition of a bias term:\n",
    "$$\n",
    "\\mathcal{G}\\theta(a(\\Xi))(x) = b\\odot t + b_0  = \\sum_{k=1}^p b_k(a(\\Xi)) \\cdot t_k(x) + b_0,\n",
    "$$\n",
    "where $b_k(a(\\Xi))$ and $t_k(x)$ are the $k$-th components of the outputs from the branch and trunk networks, respectively, and $b_0$ is a bias term. This formulation allows the model to separately learn:\n",
    "- How the **input function** affects the solution (branch network),\n",
    "- How the **location** affects the solution (trunk network),\n",
    "\n",
    "and then combine them dynamically at inference time to produce the desired solution at any location $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06840376-228a-448d-a1ff-0eea4c53117b",
   "metadata": {},
   "source": [
    "## (2) How to Use DeepONet — The DeepONet Method\n",
    "The DeepONet method is a **supervised learning approach** that learns the mapping from an input function (e.g., a coefficient or source term) to the corresponding solution of a PDE. The procedure involves four key steps:\n",
    "\n",
    "### Step 1: Collecting Labeled Training Pairs\n",
    "\n",
    "Since DeepONet is a data-driven model, the first and most essential step is to collect a dataset of input-output pairs:\n",
    "$$\n",
    "\\mathcal{D} = \\left\\{ \\left(a^{(i)}(\\Xi), u^{(i)} \\right) \\right\\}_{i=1}^{N\\text{data}}\n",
    "$$\n",
    "- $a^{(i)}(\\Xi)$: Discretized or feature-based representation of the input function for sample i.\n",
    "- $u^{(i)}$: Corresponding solution of the PDE.\n",
    "\n",
    "#### How to get this data?\n",
    "- The input functions $a^{(i)}$ can be sampled randomly or from a predefined function space $\\mathcal{A}$.\n",
    "- The output solutions $u^{(i)}$ must be computed using accurate numerical methods (e.g., finite difference, finite element, spectral methods).\n",
    "\n",
    "### Step 2: Building the DeepONet Model\n",
    "\n",
    "To approximate the unknown operator $\\mathcal{G}$, we use the DeepONet architecture $\\mathcal{G}_\\theta$, composed of two neural networks:\n",
    "#### Trunk Network:\n",
    "- Takes spatial coordinates $x \\in \\Omega$ as input.\n",
    "- Outputs a vector $t(x) = (t_1(x), \\dots, t_p(x))$.\n",
    "- Typically implemented using a Multilayer Perceptron (MLP).\n",
    "\n",
    "#### Branch Network:\n",
    "- Takes the discretized input function $a(\\Xi)$ as input.\n",
    "- Outputs a vector $b(a) = (b_1(a), \\dots, b_p(a))$.\n",
    "- Implemented as:\n",
    "  - MLP if $a(\\Xi)$ is a low-dimensional vector.\n",
    "  - CNN if $a(\\Xi)$ is high-dimensional (e.g., image-like structure).\n",
    "\n",
    "#### Combined Output:\n",
    "The DeepONet prediction at location $x$ is given by:\n",
    "$$\n",
    "\\mathcal{G}\\theta(a(\\Xi))(x) = \\sum_{k=1}^p b_k(a(\\Xi)) \\cdot t_k(x) + b_0\n",
    "$$\n",
    "where\n",
    "- $b_0$ is a learnable bias term.\n",
    "- The model parameters $\\theta$ include all weights in the trunk and branch networks, plus $b_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5a1e-fc43-4a4c-8a1f-0525e0ce887c",
   "metadata": {},
   "source": [
    "### Step 3: Defining the Loss Function\n",
    "\n",
    "To train DeepONet, we aim to minimize the discrepancy between its predictions and the true PDE solutions across all training samples and evaluation points. This is typically formulated as a **mean squared error (MSE)** loss:\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\sum_{j=1}^{N_p} \\left| \\mathcal{G}_\\theta(a^{(i)}(\\Xi))(x_j) - u^{(i)}(x_j) \\right|^2\n",
    "$$\n",
    "- $\\{x_j\\}_{j=1}^{N_p}$: Set of collocation (evaluation) points in the domain.\n",
    "- The objective is to find optimal parameters:\n",
    "  $$\n",
    "    \\theta^* = \\arg \\min_{\\theta} L(\\theta)\n",
    "  $$\n",
    "  \n",
    "**Note:** Other loss functions (e.g., relative error, custom physics-informed losses) may also be used depending on the application.\n",
    "\n",
    "### Step 4: Training with Gradient Descent\n",
    "\n",
    "The model is trained using a gradient-based optimization algorithm such as Stochastic Gradient Descent (SGD) or Adam. The parameters are updated iteratively as:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - l_r \\nabla_\\theta L(\\theta_t)\n",
    "$$\n",
    "- $l_r$: Learning rate.\n",
    "- $t$: Current iteration (also called an epoch).\n",
    "\n",
    "Training continues until the loss converges to a satisfactory level or a maximum number of epochs is reached.\n",
    "\n",
    "###  Summary of DeepONet Usage\n",
    "|Step|Description|\n",
    "|---|---|\n",
    "| Step1|Generate training data $(a^{(i)}, u^{(i)})$ via simulation or experiment|\n",
    "| Step2|Build DeepONet with trunk and branch networks|\n",
    "| Step3|Define a loss function based on prediction error|\n",
    "| Step4|Train the model with gradient descent|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c0264-fd6f-489f-90ae-f7d9285d6765",
   "metadata": {},
   "source": [
    "## (3) When Should We Use DeepONet? — Advantages and Limitations\n",
    "\n",
    "DeepONet is a powerful tool for learning solution operators to PDEs, especially in scenarios where fast inference is required across many different inputs. However, like any method, it comes with both strengths and trade-offs.\n",
    "\n",
    "### (3.1) Advantages of DeepONet\n",
    "\n",
    "#### [1] Function-to-Function Learning\n",
    "- Unlike traditional neural networks that operate on fixed-size vectors, DeepONet is specifically designed to learn mappings between functions.\n",
    "- This makes it suitable for solving families of PDEs with varying input functions (e.g., changing coefficients or source terms).\n",
    "- \n",
    "#### [2] Fast Inference After Training\n",
    "- Once trained, DeepONet can instantly predict the solution $u(x)$ for any new input function $a(x) \\in \\mathcal{A}$ without solving the PDE again.\n",
    "- This is particularly valuable in scenarios where many queries need to be answered quickly, such as real-time simulation, control, or optimization.\n",
    "\n",
    "\n",
    "### (3.2) Limitations of DeepONet\n",
    "#### [1] Data Inefficiency\n",
    "- Training DeepONet requires a large number of labeled input-output pairs $(a, u)$.\n",
    "- Since generating high-quality solutions $u$ often involves solving the PDE numerically (e.g., with finite element or spectral methods), this process can be computationally expensive and time-consuming.\n",
    "#### [2] Poor Generalization to Unseen Inputs\n",
    "- DeepONet tends to perform poorly if the input function a during testing lies outside the distribution of the training data.\n",
    "- In other words, the method is not robust to out-of-distribution inputs, which limits its reliability in real-world settings unless the training set is very comprehensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
